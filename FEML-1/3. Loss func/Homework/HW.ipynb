{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание, реализация логрега"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "134                6.1               2.6                5.6               1.4   \n",
       "119                6.0               2.2                5.0               1.5   \n",
       "63                 6.1               2.9                4.7               1.4   \n",
       "133                6.3               2.8                5.1               1.5   \n",
       "46                 5.1               3.8                1.6               0.2   \n",
       "\n",
       "         target  \n",
       "134   virginica  \n",
       "119   virginica  \n",
       "63   versicolor  \n",
       "133   virginica  \n",
       "46       setosa  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем набор данных Ирисы:\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "data = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "\n",
    "data['target'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "\n",
    "# Убираем ненужный класс в таргете и сбрасываем индекс:\n",
    "data = data[data['target']!='setosa'].reset_index().drop('index', axis=1)\n",
    "\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "3                 5.5               2.3                4.0               1.3   \n",
       "79                7.2               3.0                5.8               1.6   \n",
       "25                6.6               3.0                4.4               1.4   \n",
       "91                6.9               3.1                5.1               2.3   \n",
       "21                6.1               2.8                4.0               1.3   \n",
       "\n",
       "    target  \n",
       "3        0  \n",
       "79       1  \n",
       "25       0  \n",
       "91       1  \n",
       "21       0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Закодируем таргет обратно\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data['target'] = label_encoder.fit_transform(data['target'])\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:,data.columns[:4]]\n",
    "y = data.loc[:,data.columns[-1:]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, penalty=\"l2\", gamma=0, fit_intercept=True, self_moment='grad'):\n",
    "        r\"\"\"\n",
    "        A simple logistic regression model fit via gradient descent on the\n",
    "        penalized negative log likelihood.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        For logistic regression, the penalized negative log likelihood of the\n",
    "        targets **y** under the current model is\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            - \\log \\mathcal{L}(\\mathbf{b}, \\mathbf{y}) = -\\frac{1}{N} \\left[\n",
    "                \\left(\n",
    "                    \\sum_{i=0}^N y_i \\log(\\hat{y}_i) +\n",
    "                      (1-y_i) \\log(1-\\hat{y}_i)\n",
    "                \\right) - R(\\mathbf{b}, \\gamma) \n",
    "            \\right]\n",
    "        \n",
    "        where\n",
    "        \n",
    "        .. math::\n",
    "        \n",
    "            R(\\mathbf{b}, \\gamma) = \\left\\{\n",
    "                \\begin{array}{lr}\n",
    "                    \\frac{\\gamma}{2} ||\\mathbf{beta}||_2^2 & :\\texttt{ penalty = 'l2'}\\\\\n",
    "                    \\gamma ||\\beta||_1 & :\\texttt{ penalty = 'l1'}\n",
    "                \\end{array}\n",
    "                \\right.\n",
    "                \n",
    "        is a regularization penalty, :math:`\\gamma` is a regularization weight, \n",
    "        `N` is the number of examples in **y**, and **b** is the vector of model \n",
    "        coefficients.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        penalty : {'l1', 'l2'}\n",
    "            The type of regularization penalty to apply on the coefficients\n",
    "            `beta`. Default is 'l2'.\n",
    "        gamma : float\n",
    "            The regularization weight. Larger values correspond to larger\n",
    "            regularization penalties, and a value of 0 indicates no penalty.\n",
    "            Default is 0.\n",
    "        fit_intercept : bool\n",
    "            Whether to fit an intercept term in addition to the coefficients in\n",
    "            b. If True, the estimates for `beta` will have `M + 1` dimensions,\n",
    "            where the first dimension corresponds to the intercept. Default is\n",
    "            True.\n",
    "        self_moment : char\n",
    "            grad or nesterov_momentum or RMSProp\n",
    "        \"\"\"\n",
    "        err_msg = \"penalty must be 'l1' or 'l2', but got: {}\".format(penalty)\n",
    "        assert penalty in [\"l2\", \"l1\"], err_msg\n",
    "        self.beta = None\n",
    "        self.gamma = gamma\n",
    "        self.penalty = penalty\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.moment = self_moment\n",
    "\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, tol=1e-7, max_iter=1e6, nesterov_coef=0.7):\n",
    "        \"\"\"\n",
    "        Fit the regression coefficients via gradient descent on the negative\n",
    "        log likelihood.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            A dataset consisting of `N` examples, each of dimension `M`.\n",
    "        y : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            The binary targets for each of the `N` examples in `X`.\n",
    "        lr : float\n",
    "            The gradient descent learning rate. Default is 1e-7.\n",
    "        max_iter : float\n",
    "            The maximum number of iterations to run the gradient descent\n",
    "            solver. Default is 1e7.\n",
    "        nesterov_coef : float\n",
    "            The number of nesterov_coef. Default is 0.8.\n",
    "        \"\"\"\n",
    "        # convert X to a design matrix if we're fitting an intercept\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        l_prev = np.inf\n",
    "        loss_hist = []\n",
    "        grad_prev = 0\n",
    "        EG = 0 \n",
    "        self.beta = np.random.rand(X.shape[1])\n",
    "        for _ in range(int(max_iter)):\n",
    "            y_pred = sigmoid(np.dot(X, self.beta))\n",
    "            loss = self._NLL(X, y, y_pred)\n",
    "            loss_hist.append(loss)\n",
    "            if l_prev - loss < tol:\n",
    "                return\n",
    "            l_prev = loss\n",
    "            if self.moment == 'grad':\n",
    "                self.beta -= lr * self._NLL_grad(X, y, y_pred)\n",
    "                \n",
    "            elif self.moment == 'nesterov_momentum':\n",
    "                self.beta -= (nesterov_coef * grad_prev + (1 - nesterov_coef) * lr * self._NLL_grad(X, y, y_pred))\n",
    "                grad_prev = nesterov_coef * grad_prev + (1 - nesterov_coef) * lr * self._NLL_grad(X, y, y_pred)\n",
    "            elif self.moment == 'RMSProp':\n",
    "                EG = nesterov_coef * EG + (1 - nesterov_coef)*self._NLL_grad(X, y, y_pred)*self._NLL_grad(X, y, y_pred)\n",
    "                self.beta -=   lr / np.sqrt(EG+tol) * self._NLL_grad(X, y, y_pred)\n",
    "                \n",
    "\n",
    "    def _NLL(self, X, y, y_pred):\n",
    "        r\"\"\"\n",
    "        Penalized negative log likelihood of the targets under the current\n",
    "        model.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\text{NLL} = -\\frac{1}{N} \\left[\n",
    "                \\left(\n",
    "                    \\sum_{i=0}^N y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)\n",
    "                \\right) - R(\\mathbf{b}, \\gamma)\n",
    "            \\right]\n",
    "        \"\"\"\n",
    "        N, M = X.shape\n",
    "        beta, gamma = self.beta, self.gamma \n",
    "        order = 2 if self.penalty == \"l2\" else 1\n",
    "        norm_beta = np.linalg.norm(beta, ord=order)\n",
    "        \n",
    "        nll = -np.log(y_pred[y.iloc[:,0] == 1]).sum() - np.log(1 - y_pred[y.iloc[:,0] == 0]).sum()\n",
    "        penalty = (gamma / 2) * norm_beta ** 2 if order == 2 else gamma * norm_beta\n",
    "        return (penalty + nll) / N\n",
    "\n",
    "    def _NLL_grad(self, X, y, y_pred):\n",
    "        \"\"\"Gradient of the penalized negative log likelihood wrt beta\"\"\"\n",
    "        N, M = X.shape\n",
    "        l1norm = lambda x: np.linalg.norm(x, 1)  # noqa: E731\n",
    "        p, beta, gamma = self.penalty, self.beta, self.gamma\n",
    "        d_penalty = gamma * beta if p == \"l2\" else gamma * np.sign(beta)\n",
    "        return -(np.dot(y.iloc[:,0] - y_pred, X) + d_penalty) / N\n",
    "   \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to generate prediction probabilities on a new\n",
    "        collection of data points.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(Z, M)`\n",
    "            A dataset consisting of `Z` new examples, each of dimension `M`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : :py:class:`ndarray <numpy.ndarray>` of shape `(Z,)`\n",
    "            The model prediction probabilities for the items in `X`.\n",
    "        \"\"\"\n",
    "        # convert X to a design matrix if we're fitting an intercept\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return sigmoid(np.dot(X, self.beta))\n",
    " \n",
    " #мои попытки написать нужные функции или скопированные из интернета\n",
    "def sigmoid(x):\n",
    "    \"\"\"The logistic sigmoid function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def log_loss(h, y):\n",
    "    loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    return loss\n",
    "def gradient_descent(X, y, w, lr, num_iter):\n",
    "    losses = []\n",
    "    for i in range(num_iter):\n",
    "        z = np.dot(X, w)\n",
    "        h = sigmoid(z)\n",
    "        gradient = np.dot(X.T, (h -y)) / len(y)\n",
    "        w -= lr * gradient\n",
    "        loss = log_loss(h, y)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return w, losses\n",
    "\n",
    "def RMSProp(X, gamma, lr=0.25, eps=0.00001):\n",
    "    Y = []\n",
    "    EG = 0\n",
    "    for x in X:\n",
    "        EG = gamma*EG + (1-gamma)*x*x\n",
    "        v = lr/np.sqrt(EG + eps)*x\n",
    "        Y.append(v)\n",
    "    return np.asarray(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model=LogisticRegression(self_moment='grad')\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26678719815247637"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_grad = model._NLL(X_test, y_test, y_pred )\n",
    "score_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нестеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "model_nesterov=LogisticRegression(self_moment='nesterov_momentum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_nesterov.fit(X_train, y_train,nesterov_coef=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nesterov = model_nesterov.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.269743322993872"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_nesterov = model_nesterov._NLL(X_test, y_test, y_pred_nesterov )\n",
    "score_nesterov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RMSProp=LogisticRegression(self_moment='RMSProp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_RMSProp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_RMSProp = model_RMSProp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4568677970632627"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_RMSProp = model_RMSProp._NLL(X_test, y_test, y_pred_RMSProp )\n",
    "score_RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
