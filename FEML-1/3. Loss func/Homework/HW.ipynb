{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание, реализация логрега"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, penalty=\"l2\", gamma=0, fit_intercept=True):\n",
    "        r\"\"\"\n",
    "        A simple logistic regression model fit via gradient descent on the\n",
    "        penalized negative log likelihood.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        For logistic regression, the penalized negative log likelihood of the\n",
    "        targets **y** under the current model is\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            - \\log \\mathcal{L}(\\mathbf{b}, \\mathbf{y}) = -\\frac{1}{N} \\left[\n",
    "                \\left(\n",
    "                    \\sum_{i=0}^N y_i \\log(\\hat{y}_i) +\n",
    "                      (1-y_i) \\log(1-\\hat{y}_i)\n",
    "                \\right) - R(\\mathbf{b}, \\gamma) \n",
    "            \\right]\n",
    "        \n",
    "        where\n",
    "        \n",
    "        .. math::\n",
    "        \n",
    "            R(\\mathbf{b}, \\gamma) = \\left\\{\n",
    "                \\begin{array}{lr}\n",
    "                    \\frac{\\gamma}{2} ||\\mathbf{beta}||_2^2 & :\\texttt{ penalty = 'l2'}\\\\\n",
    "                    \\gamma ||\\beta||_1 & :\\texttt{ penalty = 'l1'}\n",
    "                \\end{array}\n",
    "                \\right.\n",
    "                \n",
    "        is a regularization penalty, :math:`\\gamma` is a regularization weight, \n",
    "        `N` is the number of examples in **y**, and **b** is the vector of model \n",
    "        coefficients.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        penalty : {'l1', 'l2'}\n",
    "            The type of regularization penalty to apply on the coefficients\n",
    "            `beta`. Default is 'l2'.\n",
    "        gamma : float\n",
    "            The regularization weight. Larger values correspond to larger\n",
    "            regularization penalties, and a value of 0 indicates no penalty.\n",
    "            Default is 0.\n",
    "        fit_intercept : bool\n",
    "            Whether to fit an intercept term in addition to the coefficients in\n",
    "            b. If True, the estimates for `beta` will have `M + 1` dimensions,\n",
    "            where the first dimension corresponds to the intercept. Default is\n",
    "            True.\n",
    "        \"\"\"\n",
    "        err_msg = \"penalty must be 'l1' or 'l2', but got: {}\".format(penalty)\n",
    "        assert penalty in [\"l2\", \"l1\"], err_msg\n",
    "        self.beta = None\n",
    "        self.gamma = gamma\n",
    "        self.penalty = penalty\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, tol=1e-7, max_iter=1e7):\n",
    "        \"\"\"\n",
    "        Fit the regression coefficients via gradient descent on the negative\n",
    "        log likelihood.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            A dataset consisting of `N` examples, each of dimension `M`.\n",
    "        y : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            The binary targets for each of the `N` examples in `X`.\n",
    "        lr : float\n",
    "            The gradient descent learning rate. Default is 1e-7.\n",
    "        max_iter : float\n",
    "            The maximum number of iterations to run the gradient descent\n",
    "            solver. Default is 1e7.\n",
    "        \"\"\"\n",
    "        # convert X to a design matrix if we're fitting an intercept\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        l_prev = np.inf\n",
    "        self.beta = np.random.rand(X.shape[1])\n",
    "        for _ in range(int(max_iter)):\n",
    "            y_pred = sigmoid(np.dot(X, self.beta))\n",
    "            loss = self._NLL(X, y, y_pred)\n",
    "            if l_prev - loss < tol:\n",
    "                return\n",
    "            l_prev = loss\n",
    "            self.beta -= lr * self._NLL_grad(X, y, y_pred)\n",
    "\n",
    "    def _NLL(self, X, y, y_pred):\n",
    "        r\"\"\"\n",
    "        Penalized negative log likelihood of the targets under the current\n",
    "        model.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\text{NLL} = -\\frac{1}{N} \\left[\n",
    "                \\left(\n",
    "                    \\sum_{i=0}^N y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)\n",
    "                \\right) - R(\\mathbf{b}, \\gamma)\n",
    "            \\right]\n",
    "        \"\"\"\n",
    "        N, M = X.shape\n",
    "        beta, gamma = self.beta, self.gamma \n",
    "        order = 2 if self.penalty == \"l2\" else 1\n",
    "        norm_beta = np.linalg.norm(beta, ord=order)\n",
    "        \n",
    "        nll = -np.log(y_pred[y == 1]).sum() - np.log(1 - y_pred[y == 0]).sum()\n",
    "        penalty = (gamma / 2) * norm_beta ** 2 if order == 2 else gamma * norm_beta\n",
    "        return (penalty + nll) / N\n",
    "\n",
    "    def _NLL_grad(self, X, y, y_pred):\n",
    "        \"\"\"Gradient of the penalized negative log likelihood wrt beta\"\"\"\n",
    "        N, M = X.shape\n",
    "        l1norm = lambda x: np.linalg.norm(x, 1)  # noqa: E731\n",
    "        p, beta, gamma = self.penalty, self.beta, self.gamma\n",
    "        d_penalty = gamma * beta if p == \"l2\" else gamma * np.sign(beta)\n",
    "        return -(np.dot(y - y_pred, X) + d_penalty) / N\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to generate prediction probabilities on a new\n",
    "        collection of data points.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(Z, M)`\n",
    "            A dataset consisting of `Z` new examples, each of dimension `M`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : :py:class:`ndarray <numpy.ndarray>` of shape `(Z,)`\n",
    "            The model prediction probabilities for the items in `X`.\n",
    "        \"\"\"\n",
    "        # convert X to a design matrix if we're fitting an intercept\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return sigmoid(np.dot(X, self.beta))\n",
    "    \n",
    "def sigmoid(x):\n",
    "    \"\"\"The logistic sigmoid function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def log_loss(h, y):\n",
    "    loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    return loss\n",
    "def gradient_descent(X, y, w, lr, num_iter):\n",
    "    losses = []\n",
    "    for i in range(num_iter):\n",
    "        z = np.dot(X, w)\n",
    "        h = sigmoid(z)\n",
    "        gradient = np.dot(X.T, (h -y)) / len(y)\n",
    "        w -= lr * gradient\n",
    "        loss = log_loss(h, y)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 1, 2, 2, 1, 1, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression()\n",
    "X = np.random.randint(1,5,10)\n",
    "Y = np.random.randint(0,2,10)\n",
    "X_test =  np.random.randint(1,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 2 3 1 2 2 4 4] [1 1 1 1 0 1 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68377033, 0.62083687, 0.62083687, 0.79038644, 0.68377033,\n",
       "       0.68377033, 0.68377033, 0.68377033, 0.68377033, 0.62083687])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
